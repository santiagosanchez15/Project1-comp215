{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWgdMRQ+L90mzW6eQFQ0is",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagosanchez15/Project1-comp215/blob/main/Comp_215_prject_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1 comp 215\n",
        "\n",
        "**Author:** Santiago Sanchez Covarrubias\n",
        "\n",
        "**resources**: *Think python, Claude.ai*\n",
        "  - https://allendowney.github.io/ThinkPython/. Think python URL\n",
        "\n",
        "\n",
        "**Objectives**\n",
        "- The creation of a SLM capable of to predict the third word\n",
        "\n",
        "**Project description**\n",
        "\n",
        "The project will develop a SLM capable of predicting the third word.\n",
        "\n",
        "This project will be focus not only on developing the SLM but also on documenting the process.\n",
        "Starting by adding different sections, that at the end of different sections will join all the pieces together.\n",
        "\n",
        "After the SLM has been built with feeded data, the final SLM will be created by inhereting everything from the first one, the difference is tyhat this final version will not only take the feeded data through files but aslo through the Wikimedia REST API, the perfect source for thousand of wirtten texts.\n",
        "\n",
        "At the end of all the documentation the full code will be available.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K4B-5Ynb7MuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api"
      ],
      "metadata": {
        "id": "ZyfxUEY1E5zo",
        "outputId": "8d4d959f-8360-4384-cc6b-a12657258f7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.9.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2026.1.4)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.9.0-py3-none-any.whl size=15422 sha256=a75750c20014242166be9bf25a924741cd500ccf72a9c1e767289d328dd0a8e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/22/bd/5181c75f59d48538eb0c0f3246ac541b8a3f0bce3bfd097047\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string #used for punctuation signs\n",
        "from collections import Counter #used to merge and join two dictionaries\n",
        "import numpy as np\n",
        "\n",
        "import wikipediaapi"
      ],
      "metadata": {
        "id": "XJQVWReTwtlr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsed and clean function\n",
        "\n",
        "  - get_text:\n",
        "    - will get the text from the file\n",
        "\n",
        "  - clean_text\n",
        "    - will iterate through all the text, check if there are punctuations signs, remove and create a new list of words\n",
        "\n",
        "  - bigrams -> words\n",
        "    - check the anount aof times a word repeats after an specific word, then added to dictionary\n",
        "      \"Hello how are you\"\n",
        "      1 - (Hello how)\n",
        "      2- (how are)\n",
        "      3- (are you)\n",
        "      amount of times the second word will come after the first one\n",
        "      {\"Hello\", {\"how\": 2, \"this\": 4}} etc.\n",
        "\n",
        "\n",
        "  - trigrams -> 3 words\n",
        "    - same like bigrams but instead teh combination of 3 words\n",
        "      next two words plus the word checking such as\n",
        "      \"The castle is big and made of stone\"\n",
        "      1 - (the castle is)\n",
        "      2- (castle is big)\n",
        "      3- (is big and)\n",
        "      aount of times the next two words will come after the first one\n",
        "      {'The': {\"castle is\", 3}\n",
        "      {\"The': {\"red carpet\", 2}\n",
        "      etc, next two words following the first one there fore key can be tuple"
      ],
      "metadata": {
        "id": "_Ic4JweXDpLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean text\n",
        "two functions, clean a text from a file and another one to clean the text from a string"
      ],
      "metadata": {
        "id": "J2hEw9E_NIoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDhAz7987LpL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def clean_text_from_file(file_name: str) -> list:\n",
        "  '''from a given file returns a list of strings with the texted parsed and cleaned '''\n",
        "\n",
        "  with open(file_name, 'r') as text: #open file given\n",
        "    return [word.strip(string.punctuation).lower()  for line in text for word in line.split() if word.strip(string.punctuation)] #iterate through each word and strip to get clean word\n",
        "\n",
        "assert clean_text_from_file('sample.txt')[:2] == ['hello', 'world']\n",
        "assert clean_text_from_file('sample.txt')[-1] == 'wonderful'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(string_text: str) -> list:\n",
        "  '''Returns list of word cleaned '''\n",
        "  return [word.strip(string.punctuation).lower() for word in string_text.split() if word.strip(string.punctuation)]"
      ],
      "metadata": {
        "id": "Tb73VLIR01mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = \"Hello! my? friend is you!!!!!\"\n",
        "print(clean_text(list1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH8aKXuE19RA",
        "outputId": "d5828d15-8b07-4d91-9ae8-ab030b0be849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'my', 'friend', 'is', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_bigram\n",
        "get Bigram from given word so for example if input is: hello my name is santiago then the output would be (hello, my), (my, name), (name, is), (is, santiago)"
      ],
      "metadata": {
        "id": "mQNLcVebixVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bigram(list_word: list) -> list:\n",
        "  '''Return list of bigrams '''\n",
        "\n",
        "  return list(zip(list_word[:-1], list_word[1:]))"
      ],
      "metadata": {
        "id": "_CNGk9Zti7kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit testing\n",
        "\n",
        "assert get_bigram(['hello', 'my', 'name', 'is', 'santiago']) == [('hello', 'my'), ('my', 'name'), ('name', 'is'), ('is', 'santiago')]"
      ],
      "metadata": {
        "id": "XYyJsdNhjNE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_trigram\n",
        "get trigram from given word\n",
        "so for example if input is:\n",
        "hello my name is santiago\n",
        "then the output would be\n",
        "(hello, my, name), (my, name, is), (name, is, santiago)"
      ],
      "metadata": {
        "id": "wwk3vmqIABDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trigrams(list_word: list) -> list:\n",
        "  '''Returns list of trigrams '''\n",
        "\n",
        "  return list(zip(list_word[:-2], list_word[1:-1], list_word[2:]))"
      ],
      "metadata": {
        "id": "V3BYqtMYAVMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing unit\n",
        "\n",
        "print(get_trigrams('hello my name is santiago'.split()))\n",
        "assert get_trigrams('hello my name is santiago'.split()) == [('hello', 'my', 'name'), ('my', 'name', 'is'), ('name', 'is', 'santiago')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aSbAYqNArP9",
        "outputId": "0c7a426d-76ea-490e-f30c-61fd97b505fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 'my', 'name'), ('my', 'name', 'is'), ('name', 'is', 'santiago')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge_dictionaries\n",
        "\n",
        "function that will take two dictionaries and merge the two of them adding the elements and counting the bigrams"
      ],
      "metadata": {
        "id": "cOskLr1qoZhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dictionaries(old_dict: dict, new_dict: dict) -> dict:\n",
        "  '''Returns dictionary with updated values '''\n",
        "\n",
        "  all_keys = set(old_dict.keys() | new_dict.keys()) #we crate a set to get all the keys of both dicitionaries merging them\n",
        "  result = {}\n",
        "\n",
        "  for key in all_keys: #iterate thorugh they keys of both dictionaries\n",
        "    counter1 = Counter(old_dict.get(key, {})) #use Counter function to get attributes\n",
        "    counter2 = Counter(new_dict.get(key, {}))\n",
        "    result[key] = dict(counter1 + counter2) #add the attributes to a new dictionary form given key\n",
        "\n",
        "  return result\n",
        "\n"
      ],
      "metadata": {
        "id": "PYODKO_mojSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict1 = {(\"hello\", 'how'): {\"are\": 1, \"is\": 2}, (\"apple\", 'is'): {\"a\": 1, \"healthy\": 2}, (\"apples\", \"are\"): {\"my\": 1, \"taste\": 2}}\n",
        "dict2 = {(\"hello\", 'how'): {\"are\": 3}, \"pear\": {\"yummy\": 1}}\n",
        "new_dict = merge_dictionaries(dict1, dict2)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01oTqBwmpOmN",
        "outputId": "a9d3cd8f-b136-46bb-9842-c45463157edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('apple', 'is'): {'a': 1, 'healthy': 2}, ('hello', 'how'): {'are': 4, 'is': 2}, 'pear': {'yummy': 1}, ('apples', 'are'): {'my': 1, 'taste': 2}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word_frequency\n",
        "this function will take a dictionary and a given list of bigrams to update the dictionary given with the values corresponding to the frequency of the words appearance"
      ],
      "metadata": {
        "id": "oYjUE6SpYJTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency(trigrams:list) -> dict:\n",
        "  '''Returns a dictionary with updated frequency of words '''\n",
        "\n",
        "  new_dict = {}\n",
        "  for key1, key2, value in trigrams: #iterate trhough every element in the list of bigrams tuples\n",
        "    if (key1, key2) not in new_dict: new_dict[(key1, key2)] = {} #create a new key if the key doenst exist\n",
        "    if value not in new_dict[(key1, key2)]: new_dict[(key1, key2)][value] = 1 #give a value of 1 if the value doesnt exist\n",
        "    else: new_dict[(key1, key2)][value] += 1 #update the value once the word is found\n",
        "\n",
        "\n",
        "  return new_dict\n",
        "#try function\n",
        "\n"
      ],
      "metadata": {
        "id": "wCNE24uFZUQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try function above\n",
        "tuple_t = ((1,2,3), (4,3,2))\n",
        "print(word_frequency(tuple_t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k1ioSL7lzKM",
        "outputId": "256ef09c-b3ee-4ab5-d777-ba4151d8ceee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(1, 2): {3: 1}, (4, 3): {2: 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#word_frequency_from_file\n",
        "\n",
        "Lets join all the functions together into a single function\n",
        "it will take a file name as a paramter and return the dictionary that will be used to feed the model"
      ],
      "metadata": {
        "id": "C_tTtTWftG2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency_from_file(file_name: str, old_dict: dict) -> dict:\n",
        "  '''Updates dictionary of frequencies from a given file '''\n",
        "\n",
        "  text = clean_text_from_file(file_name) #get the clean text as a list\n",
        "  trigrams = get_trigrams(text) #get bigrams form the zip function\n",
        "  frequency = word_frequency(trigrams) #get a new dictioanry of frequencies\n",
        "  return merge_dictionaries(old_dict, frequency) #returns the updated dictionary\n"
      ],
      "metadata": {
        "id": "NinGAW10tgBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test function\n",
        "new_dict = {}\n",
        "list_files = ['text1.txt', 'text2.txt']\n",
        "for file in list_files:\n",
        "  new_dict = word_frequency_from_file(file, new_dict)\n",
        "\n",
        "print(new_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZAbH4Q9uqkn",
        "outputId": "859eca21-f1e4-48dc-fd0c-3d7506915738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('my', 'friend'): {'is': 1}, ('name', 'is'): {'santigo': 1}, ('my', 'name'): {'is': 1}, ('friend', 'is'): {'you': 1}, ('hello', 'my'): {'name': 1, 'friend': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets test the function with more complex files"
      ],
      "metadata": {
        "id": "zIprjybf57A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test function complex files\n",
        "new_dict = {}\n",
        "new_file_list = ['trigram_test1.txt', 'trigram_test2.txt']\n",
        "for complex_file in new_file_list:\n",
        "  new_dict = word_frequency_from_file(complex_file, new_dict)\n",
        "\n",
        "new_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk6jmI3i5-LR",
        "outputId": "0dd60da9-63f2-4f5a-b51d-3f6b0a695140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('sit', 'down'): {'i': 1},\n",
              " ('cat', 'sat'): {'on': 2},\n",
              " ('the', 'dog'): {'sat': 2, 'lie': 1},\n",
              " ('chair', 'and'): {'the': 1},\n",
              " ('the', 'blue'): {'car': 2, 'truck': 1},\n",
              " ('drives', 'fast'): {'the': 1},\n",
              " ('sister', 'likes'): {'the': 1},\n",
              " ('likes', 'the'): {'blue': 1, 'red': 1},\n",
              " ('on', 'the'): {'mat': 1, 'floor': 1, 'chair': 1, 'bed': 1},\n",
              " ('bed', 'i'): {'saw': 1},\n",
              " ('drives', 'wildly'): {'my': 1},\n",
              " ('friend', 'likes'): {'the': 1},\n",
              " ('blue', 'car'): {'drives': 1, 'my': 1},\n",
              " ('wildly', 'my'): {'friend': 1},\n",
              " ('but', 'the'): {'red': 1},\n",
              " ('car', 'my'): {'sister': 1},\n",
              " ('and', 'the'): {'dog': 1},\n",
              " ('the', 'red'): {'car': 1, 'truck': 2},\n",
              " ('cat', 'sit'): {'down': 1},\n",
              " ('slow', 'the'): {'red': 1},\n",
              " ('dog', 'lie'): {'down': 1},\n",
              " ('the', 'chair'): {'and': 1},\n",
              " ('sat', 'on'): {'the': 4},\n",
              " ('my', 'friend'): {'likes': 1},\n",
              " ('blue', 'truck'): {'drives': 1},\n",
              " ('mat', 'the'): {'dog': 1},\n",
              " ('the', 'bed'): {'i': 1},\n",
              " ('drives', 'slow'): {'the': 1},\n",
              " ('the', 'cat'): {'sat': 2, 'sit': 1},\n",
              " ('red', 'car'): {'drives': 1},\n",
              " ('carefully', 'but'): {'the': 1},\n",
              " ('the', 'mat'): {'the': 1},\n",
              " ('dog', 'sat'): {'on': 2},\n",
              " ('drives', 'carefully'): {'but': 1},\n",
              " ('fast', 'the'): {'blue': 1},\n",
              " ('truck', 'drives'): {'slow': 1, 'wildly': 1},\n",
              " ('the', 'floor'): {'the': 1},\n",
              " ('saw', 'the'): {'cat': 1, 'dog': 1},\n",
              " ('down', 'i'): {'saw': 1},\n",
              " ('red', 'truck'): {'drives': 1},\n",
              " ('floor', 'the'): {'cat': 1},\n",
              " ('my', 'sister'): {'likes': 1},\n",
              " ('car', 'drives'): {'fast': 1, 'carefully': 1},\n",
              " ('i', 'saw'): {'the': 2}}"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word_frequency_from text\n",
        "Ofcourse at this point of the project we can get the word frequency from file, bu twhat if we just want to copy and paste. well that is easy"
      ],
      "metadata": {
        "id": "3JIOmJkWy2_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_freqeuncy_from_text(given_string: str, old_dict: dict) -> dict:\n",
        "  '''Updates dictionary of frequencies from a given text '''\n",
        "\n",
        "  new_list = clean_text(given_string)\n",
        "  trigrams = get_trigrams(new_list) #get bigrams form the zip function\n",
        "  frequency = word_frequency(trigrams) #get a new dictioanry of frequencies\n",
        "  return merge_dictionaries(old_dict, frequency) #returns the updated dictionary"
      ],
      "metadata": {
        "id": "3VB58C0dzaSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = ['The blue car drives fast. The blue truck drives slow. The red car drives carefully, but the red truck drives wildly. My friend likes the blue car. My sister likes the red truck.', 'The cat sat on the mat. The dog sat on the floor. The cat sat on the chair, and the dog sat on the bed. I saw the cat sit down. I saw the dog lie down.']\n",
        "dict1 = {}\n",
        "for text in list1:\n",
        "  dict1 = word_freqeuncy_from_text(text, dict1)\n",
        "dict1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BexleW3Xz4Nw",
        "outputId": "8c5451f2-3997-4dbe-912a-7309529281f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('sit', 'down'): {'i': 1},\n",
              " ('cat', 'sat'): {'on': 2},\n",
              " ('the', 'dog'): {'sat': 2, 'lie': 1},\n",
              " ('chair', 'and'): {'the': 1},\n",
              " ('the', 'blue'): {'car': 2, 'truck': 1},\n",
              " ('drives', 'fast'): {'the': 1},\n",
              " ('sister', 'likes'): {'the': 1},\n",
              " ('likes', 'the'): {'blue': 1, 'red': 1},\n",
              " ('on', 'the'): {'mat': 1, 'floor': 1, 'chair': 1, 'bed': 1},\n",
              " ('bed', 'i'): {'saw': 1},\n",
              " ('drives', 'wildly'): {'my': 1},\n",
              " ('friend', 'likes'): {'the': 1},\n",
              " ('blue', 'car'): {'drives': 1, 'my': 1},\n",
              " ('wildly', 'my'): {'friend': 1},\n",
              " ('but', 'the'): {'red': 1},\n",
              " ('car', 'my'): {'sister': 1},\n",
              " ('and', 'the'): {'dog': 1},\n",
              " ('the', 'red'): {'car': 1, 'truck': 2},\n",
              " ('cat', 'sit'): {'down': 1},\n",
              " ('slow', 'the'): {'red': 1},\n",
              " ('dog', 'lie'): {'down': 1},\n",
              " ('the', 'chair'): {'and': 1},\n",
              " ('sat', 'on'): {'the': 4},\n",
              " ('my', 'friend'): {'likes': 1},\n",
              " ('blue', 'truck'): {'drives': 1},\n",
              " ('mat', 'the'): {'dog': 1},\n",
              " ('the', 'bed'): {'i': 1},\n",
              " ('drives', 'slow'): {'the': 1},\n",
              " ('the', 'cat'): {'sat': 2, 'sit': 1},\n",
              " ('red', 'car'): {'drives': 1},\n",
              " ('carefully', 'but'): {'the': 1},\n",
              " ('the', 'mat'): {'the': 1},\n",
              " ('drives', 'carefully'): {'but': 1},\n",
              " ('dog', 'sat'): {'on': 2},\n",
              " ('fast', 'the'): {'blue': 1},\n",
              " ('truck', 'drives'): {'slow': 1, 'wildly': 1},\n",
              " ('the', 'floor'): {'the': 1},\n",
              " ('saw', 'the'): {'cat': 1, 'dog': 1},\n",
              " ('down', 'i'): {'saw': 1},\n",
              " ('red', 'truck'): {'drives': 1},\n",
              " ('floor', 'the'): {'cat': 1},\n",
              " ('my', 'sister'): {'likes': 1},\n",
              " ('car', 'drives'): {'fast': 1, 'carefully': 1},\n",
              " ('i', 'saw'): {'the': 2}}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#what next?\n",
        "since we are able to get the frequency of the word and what comes next, we need to do a couple of more things to....\n",
        "Next fucntions will be:\n",
        "  - list of possible next word -> returns a list of key of the trigram\n",
        "\n",
        "  - get_probability -> get weight word and divide by total weight -> probability\n",
        "\n",
        "  - get_weight_word -> weight / total weight using numpy to assign weight return dicitonary key = word, value = probability\n",
        "\n",
        "  - get_word -> using NumPy pseduo-random numbers, get word based on the different possibilites\n"
      ],
      "metadata": {
        "id": "vu1uLxVdLMlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#List_possible_words\n",
        "\n",
        "returns a list of all the possible word that can be chosen independently from the weight"
      ],
      "metadata": {
        "id": "tqCy-llKQoJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_possible_words(dict_weights: dict, bigram: tuple) -> list:\n",
        "  '''Returns all possible word based on bigram '''\n",
        "\n",
        "  return list(dict_weights[bigram].keys())"
      ],
      "metadata": {
        "id": "eIHOsYwWQ3qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dict = {}\n",
        "new_file_list = ['trigram_test1.txt', 'trigram_test2.txt']\n",
        "for complex_file in new_file_list:\n",
        "  new_dict = word_frequency_from_file(complex_file, new_dict)\n",
        "\n",
        "new_dict\n",
        "get_all_possible_words(new_dict, ('on', 'the') )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCYMZUkVilU",
        "outputId": "2c95b287-e919-4c0b-c9b8-b3adc2560981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mat', 'floor', 'chair', 'bed']"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_probability function\n",
        "based on the total of weight return the proability of the word occurring\n",
        "for example if total weight = 25 and my word occurece 5 times that 5/25 = 0.2"
      ],
      "metadata": {
        "id": "57fOI9uVYh4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probability(num_appearance: int, total_weight: int) -> int:\n",
        "  '''Returns probability of the word to appear '''\n",
        "  return num_appearance / total_weight"
      ],
      "metadata": {
        "id": "_6zoxrfTZN3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test function above\n",
        "assert round(get_probability(5, 140), 5) == 0.03571\n",
        "assert round(get_probability(0, 150), 5) == 0\n",
        "assert round(get_probability(10, 10), 5) == 1\n",
        "assert round(get_probability(20, 154), 5) == 0.12987"
      ],
      "metadata": {
        "id": "lxTSKSN8aNbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_total_weight\n",
        "iterate trhough each value to get the toal weight, return int of weight"
      ],
      "metadata": {
        "id": "qF41y-HngG7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_total_weight(dictionary : dict, key: tuple) -> int:\n",
        "  '''return total weight for given key '''\n",
        "\n",
        "  return sum((value for value in dictionary[key].values()))"
      ],
      "metadata": {
        "id": "19GVi9RWgNcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing unit\n",
        "\n",
        "dictionary = {('hello', \"how\"): {'you': 5, \"are\": 3}, ('i', 'am'): {'your': 5, \"santiago\": 3, \"my\": 6}}\n",
        "assert get_total_weight(dictionary, ('hello', 'how')) == 8\n",
        "assert get_total_weight(dictionary, ('i', 'am')) == 14"
      ],
      "metadata": {
        "id": "z79SZ1VbgwRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#get_weighted_words\n",
        "returns list of probability of the values given, respect to the key"
      ],
      "metadata": {
        "id": "JbhfSNdYd1Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "I304cV-WblFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_words(dictionary: dict, key: tuple) -> list:\n",
        "  '''Returns a list of tuple word, dictionary '''\n",
        "\n",
        "  total_weight = get_total_weight(dictionary, key) #get total sum of the weight\n",
        "\n",
        "  #generator expression to get a list of tuples that will hold the word and the total weight\n",
        "  return list(((word, get_probability(weight, total_weight)) for word, weight in dictionary[key].items() ))"
      ],
      "metadata": {
        "id": "asWj4Aw8fKe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing units\n",
        "dictionary = {('hello', \"how\"): {'you': 5, \"are\": 3}, ('i', 'am'): {'your': 5, \"santiago\": 3, \"my\": 6}}\n",
        "assert get_weighted_words(dictionary, ('hello', 'how')) == [('you', 0.625), ('are', 0.375)]\n",
        "assert get_weighted_words(dictionary, ('i', 'am')) == [('your', 0.35714285714285715), ('santiago', 0.21428571428571427), ('my', 0.42857142857142855 )]"
      ],
      "metadata": {
        "id": "iAItCbN6flG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#predict_word\n",
        "form the list given, return the word by given probability"
      ],
      "metadata": {
        "id": "VnG-sdGTmhKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_word(dictionary: dict, key: tuple) -> str:\n",
        "  '''Returns string for given probability '''\n",
        "\n",
        "  if key not in dictionary: return None #handle case where input is not valid\n",
        "  words, probability = zip(*get_weighted_words(dictionary, key)) # unoack the values with * given each index to each variable\n",
        "  return str(np.random.default_rng().choice(words, p = probability))"
      ],
      "metadata": {
        "id": "3zp7PWf7mrS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit test\n",
        "\n",
        "new_dict = {}\n",
        "new_file_list = ['trigram_test1.txt', 'trigram_test2.txt']\n",
        "for complex_file in new_file_list:\n",
        "  new_dict = word_frequency_from_file(complex_file, new_dict)\n",
        "\n",
        "print(predict_word(new_dict, ('on', 'the')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVJAqMNBcGV4",
        "outputId": "030cfd06-a4ca-42dd-b95b-8de808fcd2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#From input to list\n",
        "get string from input and get trigram, so it can be looked later\n",
        "by the word frequency"
      ],
      "metadata": {
        "id": "tsDIMZsThjxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_to_prediction(dictionary: dict, input: str) -> str:\n",
        "  '''get input and return prediction '''\n",
        "\n",
        "  input_clean = clean_text(input)\n",
        "  if len(input_clean) <= 1: return None #handle case where not enough information is given\n",
        "  bigram = get_bigram(input_clean)[-1] #in case the user gives more than two words only get the last two from the input\n",
        "\n",
        "  return predict_word(dictionary, bigram)\n"
      ],
      "metadata": {
        "id": "ju-SC_QjiXiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit testing\n",
        "\n",
        "new_dict = {}\n",
        "new_file_list = ['trigram_test1.txt', 'trigram_test2.txt']\n",
        "for complex_file in new_file_list:\n",
        "  new_dict = word_frequency_from_file(complex_file, new_dict)\n",
        "\n",
        "print(input_to_prediction(new_dict, ('on the')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd08eLQhlv7I",
        "outputId": "a0fa8e14-3a73-40c8-e535-0434a52be4df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now what\n",
        "now that that im able to predict my thrid word, whats next, well, in that case i have first to be able to feed my SLM, since i will be limited to only the words that are saved, lets create a class that will use the functions, and will be easier to use"
      ],
      "metadata": {
        "id": "lkrcEpgngzMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Small_Language_Model\n",
        "\n",
        "Lest join all the previous functions together and create a small language model object"
      ],
      "metadata": {
        "id": "gfl7rcwNhJWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Small_Language_Model:\n",
        "  '''Class to predict the third word of a sentence (last one) '''\n",
        "\n",
        "  def __init__(self, name = 'model'):\n",
        "    '''Initialize attributes of the instance '''\n",
        "\n",
        "    self.name = name\n",
        "    self.dictionary_weight = {}\n",
        "\n",
        "  def __repr__(self):\n",
        "    '''print representation of model '''\n",
        "\n",
        "    return f\"{self.name} is a small languge model that holds {len(self.dictionary_weight.keys())} keys, feed the model more to have better predictions\"\n",
        "\n",
        "  def get_dict(self):\n",
        "    '''returns dictionary of weights '''\n",
        "\n",
        "    return self.dictionary_weight\n",
        "\n",
        "  #Functions to clean and parse the text\n",
        "\n",
        "  @staticmethod\n",
        "  def clean_text_from_file(file_name: str) -> list:\n",
        "    '''from a given file returns a list of strings with the texted parsed and cleaned '''\n",
        "\n",
        "    with open(file_name, 'r') as text: #open file given\n",
        "      #iterate through each word and strip to get clean word\n",
        "      return [word.strip(string.punctuation).lower()  for line in text for word in line.split() if word.strip(string.punctuation)]\n",
        "\n",
        "  @staticmethod\n",
        "  def clean_text(string_text: str) -> list:\n",
        "    '''Returns list of word cleaned '''\n",
        "\n",
        "    return [word.strip(string.punctuation).lower() for word in string_text.split() if word.strip(string.punctuation)]\n",
        "\n",
        "\n",
        "  #Functions to get trigrams and bigrams\n",
        "\n",
        "  @staticmethod\n",
        "  def get_bigram(list_word: list) -> list:\n",
        "    '''Return list of bigrams '''\n",
        "\n",
        "    return list(zip(list_word[:-1], list_word[1:]))\n",
        "\n",
        "  @staticmethod\n",
        "  def get_trigrams(list_word: list) -> list:\n",
        "    '''Returns list of trigrams '''\n",
        "\n",
        "    return list(zip(list_word[:-2], list_word[1:-1], list_word[2:]))\n",
        "\n",
        "\n",
        "  #function to merge the dictionaries\n",
        "\n",
        "  @staticmethod\n",
        "  def merge_dictionaries(old_dict: dict, new_dict: dict) -> dict:\n",
        "    '''Returns dictionary with updated values '''\n",
        "\n",
        "    all_keys = set(old_dict.keys() | new_dict.keys()) #we crate a set to get all the keys of both dicitionaries merging them\n",
        "    result = {}\n",
        "\n",
        "    for key in all_keys: #iterate thorugh they keys of both dictionaries\n",
        "      counter1 = Counter(old_dict.get(key, {})) #use Counter function to get attributes\n",
        "      counter2 = Counter(new_dict.get(key, {}))\n",
        "      result[key] = dict(counter1 + counter2) #add the attributes to a new dictionary form given key\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "  #Function to get the word frequency\n",
        "\n",
        "  @staticmethod\n",
        "  def word_frequency(trigrams:list) -> dict:\n",
        "    '''Returns a dictionary with updated frequency of words '''\n",
        "\n",
        "    new_dict = {}\n",
        "    for key1, key2, value in trigrams: #iterate trhough every element in the list of bigrams tuples\n",
        "      if (key1, key2) not in new_dict: new_dict[(key1, key2)] = {} #create a new key if the key doenst exist\n",
        "      if value not in new_dict[(key1, key2)]: new_dict[(key1, key2)][value] = 1 #give a value of 1 if the value doesnt exist\n",
        "      else: new_dict[(key1, key2)][value] += 1 #update the value once the word is found\n",
        "\n",
        "\n",
        "    return new_dict\n",
        "\n",
        "  def word_frequency_from_file(self, file_name: str):\n",
        "    '''Updates dictionary of frequencies from a given file '''\n",
        "\n",
        "    text = self.clean_text_from_file(file_name) #get the clean text as a list\n",
        "    trigrams = self.get_trigrams(text) #get bigrams form the zip function\n",
        "    frequency = self.word_frequency(trigrams) #get a new dictioanry of frequencies\n",
        "    self.dictionary_weight = self.merge_dictionaries(self.dictionary_weight, frequency) #returns the updated dictionary\n",
        "\n",
        "  def word_frequency_from_text(self, given_string: str):\n",
        "    '''Updates dictionary of frequencies from a given text '''\n",
        "\n",
        "    new_list = self.clean_text(given_string)\n",
        "    trigrams = self.get_trigrams(new_list) #get bigrams form the zip function\n",
        "    frequency = self.word_frequency(trigrams) #get a new dictioanry of frequencies\n",
        "    self.dictionary_weight = self.merge_dictionaries(self.dictionary_weight, frequency) #returns the updated dictionary\n",
        "\n",
        "\n",
        "  #Function of possible outcomes\n",
        "\n",
        "\n",
        "  def get_all_possible_words(self, bigram: tuple) -> list:\n",
        "    '''Returns all possible word based on bigram '''\n",
        "\n",
        "    return list(self.dictionary_weight[bigram].keys())\n",
        "\n",
        "\n",
        "  #Probability functions\n",
        "\n",
        "  @staticmethod\n",
        "  def get_probability(num_appearance: int, total_weight: int) -> int:\n",
        "    '''Returns probability of the word to appear '''\n",
        "    return num_appearance / total_weight\n",
        "\n",
        "  def get_total_weight(self, key: tuple) -> int:\n",
        "    '''return total weight for given key '''\n",
        "\n",
        "    return sum((value for value in self.dictionary_weight[key].values()))\n",
        "\n",
        "  def get_weighted_words(self, key: tuple) -> list:\n",
        "    '''Returns a list of tuple word, dictionary '''\n",
        "\n",
        "    total_weight = self.get_total_weight(key) #get total sum of the weight\n",
        "\n",
        "    #generator expression to get a list of tuples that will hold the word and the total weight\n",
        "    return list(((word, self.get_probability(weight, total_weight)) for word, weight in self.dictionary_weight[key].items() ))\n",
        "\n",
        "\n",
        "  #Functions to predict the third word\n",
        "\n",
        "\n",
        "  def input_to_prediction(self, key: tuple) -> str:\n",
        "    '''Returns string for given probability '''\n",
        "\n",
        "    if key not in self.dictionary_weight: return None #handle case where input is not valid\n",
        "    words, probability = zip(*self.get_weighted_words(key)) # unoack the values with * given each index to each variable\n",
        "    return str(np.random.default_rng().choice(words, p = probability))\n",
        "\n",
        "  def predict_word(self, input: str) -> str:\n",
        "    '''get input and return prediction '''\n",
        "\n",
        "    input_clean = self.clean_text(input)\n",
        "    if len(input_clean) <= 1: return None #handle case where not enough information is given\n",
        "    bigram = self.get_bigram(input_clean)[-1] #in case the user gives more than two words only get the last two from the input\n",
        "\n",
        "    return self.input_to_prediction( bigram)\n"
      ],
      "metadata": {
        "id": "stoDPgfzmps0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit testing\n",
        "\n",
        "chatgpt = Small_Language_Model('Guerra')\n",
        "#Remember to add this files to the notebook in order to run the following code\n",
        "files = [\n",
        "    'feeding_and_evaluating_models.txt',\n",
        "    'neural_network_fundamentals.txt',\n",
        "    'probability_and_statistics.txt',\n",
        "    'text_preprocessing_nlp.txt',\n",
        "    'training_language_models.txt',\n",
        "    'the_model_who_learned_to_speak.txt',\n",
        "    'hello_how_the_team_spoke.txt'\n",
        "    ]\n",
        "\n",
        "for _ in files:\n",
        "  chatgpt.word_frequency_from_file(_)\n",
        "\n",
        "print(chatgpt)\n",
        "print(chatgpt.predict_word('santiago hello how'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X6K4gaVqHQI",
        "outputId": "69c63d0b-9043-439e-b3ee-17606ab6c3e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guerra is a small languge model that holds 7325 keys, feed the model more to have better predictions\n",
            "did\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next step\n",
        "Now that our model can predict the next word, now we have to find an easier way to feed our model since the prediction swill be better increasing the amount of data that we feed to the model....\n",
        "\n",
        "because of this, lets try to use an API (application programming interface) from wikipedia becasue theres is no better please than wikipedia to get free data"
      ],
      "metadata": {
        "id": "lRLvk7a7E_Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki = wikipediaapi.Wikipedia(user_agent=('MyLanguageModel/1.0 (your@email.com)') ,language='en')\n",
        "page = wiki.page('Machine learning')\n",
        "chatgpt.word_frequency_from_text(page.text)\n",
        "print(chatgpt)\n"
      ],
      "metadata": {
        "id": "-dzNR4z3Fpm4",
        "outputId": "0938b34c-793f-4214-d586-461ff436d7b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guerra is a small languge model that holds 13574 keys, feed the model more to have better predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inheritance\n",
        "Now that we learned that with this API we can give any wikipedia website, then we can create a second class, that will inherite evrything form the Small_Language_Model, and we'll call it SLM,\n",
        "  - api function to take the name of a the website to feed our model,\n",
        "  - function to predict the next word for n times in a for loop, the model will take its own feedback and keep predicting the next word"
      ],
      "metadata": {
        "id": "KkPjeAsLINVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SLM(Small_Language_Model):\n",
        "  '''Class that predicts the next word form text/file/api feeded input '''\n",
        "\n",
        "  def __init__(self, name):\n",
        "    '''Define attributes '''\n",
        "\n",
        "    super().__init__(name)\n",
        "    self.wiki = wikipediaapi.Wikipedia(user_agent=('MyLanguageModel/1.0 (your@email.com)') ,language='en') #create variable to access to API\n",
        "\n",
        "  def update_dictionary_wikipedia_api(self, page_title: str):\n",
        "    '''Updates dictionary using the wikipedia API '''\n",
        "\n",
        "    page = self.wiki.page(page_title) #look up for the website\n",
        "    if not page.exists():\n",
        "      print('The page was not found please look for another one')\n",
        "      return None #edge case were the page doesnt exist\n",
        "    self.word_frequency_from_text(page.text)\n",
        "\n",
        "  def keep_predicting(self, number_times: int,  input_string: str) -> str:\n",
        "    '''predict the next word n number of times '''\n",
        "\n",
        "    if number_times <= 0: return input_string + \".\" #base case for recursion\n",
        "    predicted_word = self.predict_word(input_string) #get the predicted word\n",
        "    new_string = input_string + ' ' + predicted_word #get the new string base on the given one\n",
        "\n",
        "    return self.keep_predicting(number_times -1, new_string) #call the function again but now with the new string as the input\n"
      ],
      "metadata": {
        "id": "4DHxOFPxJahu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unit testing\n",
        "chatgpt = SLM('Guerra')\n",
        "#Remember to add this files to the notebook in order to run the following code\n",
        "files = [\n",
        "    'feeding_and_evaluating_models.txt',\n",
        "    'neural_network_fundamentals.txt',\n",
        "    'probability_and_statistics.txt',\n",
        "    'text_preprocessing_nlp.txt',\n",
        "    'training_language_models.txt',\n",
        "    'the_model_who_learned_to_speak.txt',\n",
        "    'hello_how_the_team_spoke.txt'\n",
        "    ]\n",
        "\n",
        "for _ in files:\n",
        "  chatgpt.word_frequency_from_file(_)\n",
        "\n",
        "chatgpt.update_dictionary_wikipedia_api(\"machine learning\")\n",
        "print(chatgpt)\n",
        "\n",
        "chatgpt.keep_predicting(4, \"hello how are\")"
      ],
      "metadata": {
        "id": "pB6kMo7uNrVE",
        "outputId": "72ab065e-85ca-44e6-c87c-33614ef1b041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guerra is a small languge model that holds 13574 keys, feed the model more to have better predictions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello how are the predictions were getting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Questions\n",
        "• Is this an AI model? In what ways is it similar or dissimilar from AI models?\n",
        "this is not an AI model, but rather is the simpliest form of the most popula LLM, since it only predicts the next word.\n",
        "The problem comes that the model will only predict the next word depending on the data that was used to feed the data.\n",
        "\n",
        "• Could this model be useful for any real-world applications?\n",
        "It coudl be useful when you're typing something and you want extra options or dont know what other texts to use\n",
        "\n",
        "• What would be the next improvements or enhancements you would make to this model?\n",
        "I would look for an easier way to improve the data feeded and the next think is to stablish different parameters (neurons) so the model can recognize the context.\n",
        "\n"
      ],
      "metadata": {
        "id": "oQZatL8m7AJx"
      }
    }
  ]
}